# 一、语言模型的作用

语言模型最主要的作用是保证文本的语法结构，得到通顺的语句。语言模型是一种概率统计的方法，已经训练好的语言模型可以对任何一个文本给出概率，概率越高说明语法上越通顺。通过比较两句话在同一个语言模型上的概率，我们就可以得出哪一句话更通顺一些。

机器翻译和文本生成显然需要语言模型的支持。拼写纠错实际上也需要基于语言模型,毕竟拼写纠错跟语法的准确性,单词使用的准确性相关。最后一个,文本相似度计算并不需要依赖于语言模型,更多的是依赖于文本的表示和相似度计算。

举例:

今天是周日 vs 今天周日是；>>语言模型要解决的是判断P(出今天是周日) > P(今天周日是)

全民AI是趋势 vs 趋势AI是全民；>>语言模型要解决的是判断P(全民AI是趋势) > P(趋势AI是全民);

# 二、语言模型解决的问题

通过模型判断出P(出今天是周日) > P(今天周日是)；

通过模型判断出P(全民AI是趋势) > P(趋势AI是全民)；

# 三、计算语言模型的概率

The goal of probabilistic language modelling is to *calculate the probability of a sentence* of *sequence of words*:

$P(s)=P(w_1,w_2,...,w_n)$;
$$
\begin{align*}
P(A,B)&=P(A)P(B|A);\\
P(A,B,C)&=P(A)P(B,C|A)\\
&=P(A)P(B|A)P(C|B,A);\\
P(A,B,C,D)&=P(A)P(B,C,D|A)\\
&=P(A)P(B|A)P(C,D|B,A)\\
&=P(A)P(B|A)P(C|B,A)P(D|C,B,A);\\
\end{align*};\\
$$
因此，句子的概率可以表示为:
$$
\begin{align*}
P(s)&=P(w_1,w_2,w_3,...,w_n);\\
&=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_2)...P(w_n|w_1,w_2,w_3,...,w_{n-1})
\end{align*}
$$

## 3.1 chain rule for language model

$$
\begin{align*}
P(s)&=P(w_1,w_2,w_3,w_4,...,w_n)\\
&=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_2)...P(w_n|w_1,w_2,w_3,...,w_{n-1})\\
P(今天,是,春节,我们,都,休息)&=P(今天)P(是|今天)P(春节|是,今天)P(我们|今天,是,春节)P(都|今天,是,春节,我们)P(休息|今天,是,春节,我们,都)
\end{align*}
$$

# 四、马尔科夫假设

假如有一段文字:“自然语言处理 技术 广泛 应用 在 各类 AI 场景 。 自然语言处理 技术 近 几年 取得 了 飞速 的 发展。 自然语言处理 领域 的 人才 目前 还是 很 缺乏”。 下面试着计算一下条件概率 P(技术|自然语言处理) 和 P(非常|自然语言处理 技术 的 应用)值。如P(技术|自然语言处理)的值可以理解为:当出现了“自然语言处理”这个单词之后,有多少的概率紧接着出现“技术”这个单词?

2/3,0;

从上述可以看出，**当条件为较长的一段话时，一模一样的语句出现在语料库中的概率会非常小，很多时候为0，就失去了统计的意义**。那如何解决此问题呢?答案是做一些近似!

解决:**马尔科夫假设**；
$$
\begin{align*}
P(休息|今天,是,春节,我们,都)=P(休息|都);1-order马尔可夫假设\\
P(休息|今天,是,春节,我们,都)=P(休息|我们,都);2-order马尔可夫假设\\
P(休息|今天,是,春节,我们,都)=P(休息|春节,我们,都);3-order马尔可夫假设\\
...
\end{align*}
$$
因此P(s)可以计算为:
$$
\begin{align*}
P(w_1,w_2,w_3,w_4,...,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3)...P(w_n|w_{n-1});1-order马尔可夫假设\\
=P(w_1)\prod_{i=2}^{n}P(w_i|w_{i-1});\\
P(w_1,w_2,w_3,w_4,...,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)P(w_4|w_3,w_2)...P(w_n|w_{n-1},w_{n-2});2-order马尔可夫假设;\\
=P(w_1)P(w_2|w_1)\prod_{i=3}^{n}P(w_i|w_{i-1},w_{i-2});\\
P(w_1,w_2,w_3,w_4,...,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)P(w_4|w_3,w_2,w_1)...P(w_n|w_{n-1},w_{n-2},w_{n-3});3-order马尔可夫假设;\\
=P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)\prod_{i=4}^{n}P(w_i|w_{i-1},w_{i-2},w_{i-3});\\
\end{align*}
$$

# 五、语言模型训练

根据不同的马尔科夫假设(1阶、2阶、3阶。。。)，我们可以得到不同种类的语言模型。如，一阶马尔科夫假设下的语言模型为bi-gram， 二阶马尔科夫假设下的模型为tri-gram，以此类推。另外，假如前后单词之间不存在任何的依赖关系，得到的语言模型为unigram。

## 5.1 unigram

$$
\begin{align*}
P(s1) = P(今天,是,春节,我们,都,休息)=P(今天)P(是)P(春节)P(我们)P(都)P(休息);\\
P(s2) = P(今天,春节,是,都,我们,休息)=P(今天)P(春节)P(是)P(我们)P(休息)P(都);\\
P(s1) = P(s2);\\
\end{align*}
$$

问题:没有考虑句子是否通顺；

## 5.2 bigram(1-order马尔可夫假设)

$$
P(s1) = P(今天,是,春节,我们,都,休息)=P(今天)P(是|今天)P(春节|是)P(我们|春节)P(都|我们)P(休息|都);\\
P(s2) = P(今天,春节,是,都,我们,休息)=P(今天)P(春节|今天)P(是|春节)P(都|是)P(我们|都)P(休息|我们);\\
$$

## 5.3 trigram(2-order马尔可夫假设)

$$
P(s1) = P(今天,是,春节,我们,都,休息)=P(今天)P(是|今天)P(春节|是,今天)P(我们|春节,是)P(都|我们,春节)P(休息|都,我们);\\
$$

## 5.4 语言模型训练

语料库:

今天 的 天气 很好 啊

我 很 想 出去 运动

但 今天 上午 有 课程

训练营 明天 才 开始

则V=19;P(今天)=2/19;P(的)=1/19;.....

使用unigram训练:
$$
P(今天,开始,训练营,课程)=2/19*1/19*1/19*1/19;\\
P(今天,没有,训练营,课程)=2/19*0*1/19*1/19=0;\\
$$
语料库:

今天 的 天气 很好 啊

我 很 想 出去 运动

但 今天 上午 想 上课

训练营 明天 才 开始

使用bigram训练:
$$
\begin{align*}
P(今天,上午,想,出去,运动)=P(今天)P(上午|今天)P(想|上午)P(出去|想)P(运动|出去)=2/19*1/2*1*1/2*1;\\
P(今天,上午,的,天气,很好,呢)=P(今天)P(上午|今天)P(的|上午)P(天气|的)P(很好|天气)P(呢|很好)=2/19*1/2*0=0;\\
\end{align*}
$$


## 总结

- 语言模型的概率值可基于语料库来统计
- 根据使用的马尔科夫假设的不同，可以把语言模型分为unigram， bigram， trigram， ngram。。。
- 当考虑多个单词的时候，条件概率往往变得稀疏，导致大部分都变成0。

# 六、语言模型评估

## 6.1 评估指标--困惑度(perplexity)

$perplexity=2^{-x};$x:avergae log likelihood

x越大越好==>perplexity越小越好；

## 6.2 example

现有已经训练好的bigram:

P(今天)=0.002;

P(天气|今天)=0.01;

P(很好|天气)=0.1;

P(适合|很好)=0.01;

P(出去|适合)=0.02;

P(运动|出去)=0.1;

则:

**今天**;==>P(今天)=0.002;==>logP(今天)=-3+lg2;

今天**天气**；==>P(天气|今天)=0.01;==>logP(天气|今天)=-2;

今天天气**很好**；==>P(很好|天气)=0.1;==>logP(很好|天气)=-1;

今天天气很好**适合**；==>P(适合|很好)=0.01;==>logP(适合|很好)=-2;

今天天气很好适合**出去**；==>P(出去|适合)=0.02;==>logP(出去|适合)=-2+lg2;

今天天气很好适合出去**运动**;==>P(运动|出去)=0.1;==>logP(运动|出去)=-1;
$$
\begin{align*}
avg log likelihood = x = (logP(今天)+logP(天气|今天)+logP(很好|天气)+logP(适合|很好)+logP(出去|适合)+logP(运动|出去))/6;\\
perplexity=2^{-x}=...;\\
\end{align*}
$$

# 七、隐马尔可夫模型

## 7.1 概念

既是马尔可夫模型，就一定存在马尔可夫链，该马尔可夫链服从马尔可夫性质：即无记忆性。也就是说，这一时刻的状态，受且只受前一时刻的影响，而不受更往前时刻的状态的影响。

观测变量(观测序列)x,隐藏变量(状态序列)z;z-->x;有向图;

HMM参数:$\theta=(A,B,\pi)$;

$A$:状态转移概率矩阵,$shape=m*m$；$B$:发射矩阵,观测概率矩阵 ,$shape=m*|v|$；$\pi$:初始状态概率向量；

![image-20210408112906350](/Users/zenmen/Projects/courses_ml_notebook/images/image-20210408112906350.png)

什么是 HMM？既是隐形，说明这些状态是观测不到的，相应的，我们可以通过其他方式来『猜测』或是『推断』这些状态，这也是 HMM 需要解决的问题之一。

### A.例1

我女朋友现在在北京工作，而我还在法国读书。每天下班之后，她会根据天气情况有相应的活动：或是去商场购物，或是去公园散步，或是回家收拾房间。我们有时候会通电话，她会告诉我她这几天做了什么，而闲着没事的我呢，则要通过她的行为猜测这几天对应的天气最有可能是什么样子的。

以上就是一个简单的 HMM，**天气状况属于状态序列**，而**她的行为则属于观测序列**。天气状况的转换是一个马尔可夫序列。而根据天气的不同，有相对应的概率产生不同的行为。在这里，为了简化，把天气情况简单归结为晴天和雨天两种情况。雨天，她选择去散步，购物，收拾的概率分别是0.1，0.4，0.5， 而如果是晴天，她选择去散步，购物，收拾的概率分别是0.6，0.3，0.1。而天气的转换情况如下：这一天下雨，则下一天依然下雨的概率是0.7，而转换成晴天的概率是0.3；这一天是晴天，则下一天依然是晴天的概率是0.6，而转换成雨天的概率是0.4. 同时还存在一个初始概率，也就是第一天下雨的概率是0.6， 晴天的概率是0.4。
$$
A=\left[
	\begin{matrix}
	0.7 & 0.3\\
	0.4 & 0.6\\
	\end{matrix}
	\right]\\
B=\left[
\begin{matrix}
0.1 & 0.4 & 0.5\\
0.6 & 0.3 & 0.1\\
\end{matrix}
\right]\\
\pi=\left[
\begin{matrix}
0.6 & 0.4\\
\end{matrix}
\right]\\
$$
现在，重点是了解并解决HMM的三个问题。

问题1，已知整个模型，我女朋友告诉我，连续三天，她下班后做的事情分别是：散步，购物，收拾。那么，根据模型，计算**产生这些行为的概率是多少**。==>问题一，Forward Algorithm，向前算法，或者 Backward Algo，向后算法。

问题2，同样知晓这个模型，同样是这三件事，我女朋友要我猜，这三天她下班后北京的**天气是怎么样的**。这三天怎么样的天气才最有可能让她做这样的事情。==>Viterbi Algo，维特比算法

问题3，最复杂的，我女朋友只告诉我这三天她分别做了这三件事，而其他什么信息我都没有。她要我建立一个模型，晴雨转换概率，第一天天气情况的概率分布，根据天气情况她选择做某事的概率分布。（惨绝人寰）==>Baum-Welch Algo，鲍姆-韦尔奇算法（中文好绕口）

### B.例2

假设有3个盒子，每个盒子里面的球一共有10个，分别有(红，白)球:(5,5)、(4,6)和(7,3)。即状态集合Q={1,2,3}，观测集合V={红，白}。抽球产生观测序列的规则：分别对3个盒子以(0.2,0.4,0.4)的概率随机选取一个盒子，然后从该盒子中随机抽取一个球，记录颜色，然后放回。

红白球是状态变量还时观测变量？==>是观测序列；而3个盒子是状态序列；
$$
A=\left[
\begin{matrix}
0.5 & 0.2 & 0.3\\
0.3 & 0.5 & 0.2\\
0.2 & 0.3 & 0.5\\
\end{matrix}
\right]\\
即盒子1状态下，进入下一个盒子状态：盒子1,2,3的概率分别是0.5, 0.2, 0.3\\
B=\left[
\begin{matrix}
0.5 & 0.5\\
0.4 & 0.6\\
0.7 & 0.3\\
\end{matrix}
\right]\\
即在盒子1状态下，出现红，白球概率分别是0.5, 0.5\\
\pi=\left[
\begin{matrix}
0.2 & 0.4 & 0.4\\
\end{matrix}
\right]\\
$$
## 7.2 两个主要任务

(1)给定模型参数$\theta$,找出最适合的Z;==>inference/decoding;==>维特比算法

(2)估计模型参数$\theta$;==> 参数估计;==>Forward/Backward算法
A. Inference指的是预测阶段

B. 参数估计指的是模型训练阶段

C. 参数估计和Inference是两个最重要的问题

## 7.3 基于维特比算法的预测

### 7.3.1 finding best Z  with naive approach

假设观测序列=$(x1,x2,...,x_n)$,状态集合={a,b,c},也就是$z_{i}\in \{a,b,c\}$;

则状态序列=$(z_1,z_2,...,z_{n})$有$3^n$种可能；例如一种可能是$(a,a,a,b,c,a,c,b,...,b)$;

下面需要计算这个状态序列的likelihood,怎么计算？
$$
P(z_1)P(z_2|z_1)P(z_3|z_2)...P(z_n|z_{n-1})P(x_1|z_1)P(x_2|z_2)...P(x_n|z_n);\\
P(z_j|z_i)通过状态转移矩阵A获取;P(x_i|z_i)通过观测矩阵B获取;
$$

### 7.3.2 finding best Z with 维特比

原来:指数计算复杂度；

为什么能work？==>每个隐式变量z只和前后变量有关系；

<img src="/Users/zenmen/Projects/courses_ml_notebook/images/image-20210412142956000.png" alt="image-20210412142956000" style="zoom:50%;" />

这个序列的概率:

$P(z_1=2)*P(x_1|z_1=2)*P(z_2=1|z_1=2)*P(x_1|z_2=1)*...*P(z_n=j+1|z_{n-1}=j)*P(x_n|z_n=j+1)$;

维特比算法计算这个概率：

状态定义:$\delta_{k}(i)$表示到达时间k状态为i的最佳路径；

状态转移方程:
$$
\delta_{k}(i)=max(\delta_{k}(1)+logP(z_k=i|z_{k-1}=1)+logP(x_k|z_{k}=i),\\
\delta_{k}(2)+P(z_k=i|z_{k-1}=2)*P(x_k|z_{k}=i),\\
\delta_{k}(3)+P(z_k=i|z_{k-1}=3)*P(x_k|z_{k}=i),\\
...,\\
\delta_{k}(m)+P(z_k=i|z_{k-1}=m)*P(x_k|z_{k}=i));
$$
总结:
$$
\delta_{k}(i)=max(\delta_{k}(j)+logP(z_k=i|z_{k-1}=j)+logP(x_k|z_k=i)),\\
j\in\{1,2,...,m\};
$$

## 7.4 Forward/Backward算法

Forward/Backward算法在估计HMM参数中扮演很重要的角色。换句话说，在估计HMM参数过程中，会用到Forward/Backward算法的结果。所以，在这里首先给大家介绍什么是F/B算法。

Forward/Backward计算$P(z_k|x)$;

Forward计算:$P(z_k,x_{1:k})$;

Backward计算:$P(x_{k+1:n}|z_k)$;
$$
P(z_k|x)=\frac{P(z_k,x)}{P(x)}\propto P(z_k,x)=P(z_k,x_{1:k+1});\\
=
$$




















