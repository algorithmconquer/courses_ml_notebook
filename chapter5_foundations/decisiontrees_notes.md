# 一、决策树

决策树算法本身是贪心算法。所以在决策树的训练上，每一步的最优选择局限在局部；

## 1、什么是好的特征?

好的特征会减少不确定性；

不确定性--信息熵；

### 信息熵----衡量不确定性

$$
H=-\sum p_{i}logp_{i}\\
例1:arr1={0,0,1,1,0,1,1,1};H1=-\sum^2_{i=1}p_{i}logp_{i}=-(\frac{3}{8}*log\frac{3}{8}+\frac{5}{8}*log\frac{5}{8});\\
例2:arr1={0,0,0,1,0,0,0,0};H2=-(\frac{1}{8}*log\frac{1}{8}+\frac{7}{8}*log\frac{7}{8});\\
结果:H2<H1;==>数据越混乱，不确定性越大，信息熵越大；
$$

### 不确定的减少=原来的不确定性-现在的不确定性(分割后的不确定性)

不确定的减少=原来的不确定性-现在的不确定性；

现在的不确定性通过线性加权方式计算；

### 信息增益=不确定的减少

$$
IG(T,a)=H(T)-H(T|a)\\
$$

![image-20210209115218242](/Users/zenmen/Projects/courses_ml_notebook/images/image-20210209115218242.png)

原来不确定性H(T)计算
$$
11个F,5个N;\\
H(T)=-(\frac{5}{16}log\frac{5}{16}+\frac{11}{16}log\frac{11}{16});
$$

#### 单个特征信息增益计算

##### 特征Time

$$
Morning:{2F},Afternoon:{7F,4N},Night:{2F,1N};\\
H_1(T|a)=-[\frac{2}{16}*0+\frac{11}{16}*(\frac{7}{11}log\frac{7}{11}+\frac{4}{11}log\frac{4}{11})+\frac{3}{16}*(\frac{2}{3}log\frac{2}{3}+\frac{1}{3}log\frac{1}{3})];===>线性加权方式\\
G=H(T)-H_1(T|a);
$$

##### 特征MatchType

$$
Grandslam:{6F,1N},Master:{3F,3N},Friendiy:{2F,1N};\\
H_2(T|a)=-[\frac{7}{16}*(\frac{6}{7}log\frac{6}{7}+\frac{1}{7}log\frac{1}{7})+\frac{6}{16}*(\frac{3}{6}log\frac{3}{6}+\frac{3}{6}log\frac{3}{6})+\frac{3}{16}*(\frac{1}{3}log\frac{1}{3}+\frac{2}{3}log\frac{2}{3})];\\
G=H(T)-H_2(T|a);\\
$$

其他两个特征类似；

## 2、决策树的过拟合

对于决策树我们如何减少过拟合现象? 答案就是:决策树越简单越好!那什么叫更简单的决策树呢?一个重要标准是来判断决策树中节点的个数，节点个数越少说明这棵决策树就越简单。

### 2.1 什么时候可以停止分裂？

A.当一个叶节点里包含的所有样本都属于同一个类别；

B.当一个叶节点里包含所有样本的特征都相同时；

### 2.2 避免过拟合

A.设置树的最大深度；

B.当叶节点的样本个数<阈值时停止分裂；

最大深度和阈值是超参数，通过交叉验证来确定；

## 3、bagging vs boosting

### 3.1 bagging--随机森林

随机森林是经典的Bagging模型，等同于同时训练了很多棵决策树，并同时用这些决策树来做决策。

模型过拟合-->不稳定--> 方差大；

例子:邀请了7位专家，而且每一位专家在决策上犯错误的概率为0.3，那他们共同决策时最终犯错误的概率为多少呢?

注意:最终犯错误是通过投票机制(少数服从多数)来决定的；例如，有4位专家犯错误，3位正确，那最终结果为错误；
$$
总共有4种情况:4位专家犯错误,5位专家犯错误,6位专家犯错误,7位专家犯错误;\\
因此p=C^0_{7}*0.3^7+C^1_{7}*0.3^6*0.7^1+C^2_{7}*0.3^5*0.7^2+C^3_{7}*0.3^4*0.7^3\\
=0.126
$$
假如我们有N个不同的模型,而且每个模型预测时的方差为$\sigma^2$,则同时使用N个模型预测时的方差为多少?$\frac{\sigma^2}{N}$;

随机森林的预测过程无非是多棵决策树共同作出决策。比如对于分类型问题，可以通过投票的方式; 对于回归问题，则可以采用平均法则。

#### 需要考虑的点

A.只有一份训练数据；B.多棵决策树要优于单棵决策树；

#### 为什么要随机?

只有多样性(Diversity)才能保证随机森林的效果(稳定性)；如何多样性?随机;

#### 随机森林中的多样性

A.训练样本的随机性(booststrap有放回采样)；B.特征选择时的随机性；

### 3.2 boosting--gbdt,xgboost





