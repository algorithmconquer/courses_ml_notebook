# 一、文本表示基础

所谓文本的表示，其实就是研究如何把文本表示成向量或者矩阵的形式。

文本的最小单元为单词，其次为短语、句子、或者段落。我们需要懂得如何把这些表示成向量的形式。其中，单词的表示法是最基础的。另外，对于句子或者更长的文本来说，它们的表示依赖于单词的表示法。 在这里想说的一点是，单词的表示法不止一种，比如有独热编码的表示法，词向量的表示法等等。

假如使用1000维的向量来表示一个单词(用独热编码), 那最多能表示多少个单词?1000;

## 1.1 单词表示

词典:[我们，去，爬山，今天，你们，昨天，跑步]；

每个单词的one-hot表示:

我们:[**1**, 0, 0, 0, 0, 0, 0];爬山:[0, 0, **1**, 0, 0, 0, 0];跑步:[0, 0, 0, 0, 0, 0, **1**];昨天:[0, 0, 0, 0, 0, **1**, 0];

## 1.2 句子表示

词典:[我们，又，去，爬山，今天，你们，昨天，跑步]；

每个句子的one-hot(**BooleanVectorizer**)表示:

我们|今天|去|爬山:[1, 0, 1, 1, 1, 0, 0, 0];

你们|昨天|跑步:[0, 0, 0, 0, 0, 1, 1, 1];

你们|又|去|爬山|又|去|跑步=[0, 1, 1, 1, 0, 1, 0, 1];

**缺点:**没有记录单词出现的次数，没有考虑单词的重要程度；

每个句子的**CountVectorizer**表示:

我们|今天|去|爬山:[1, 0, 1, 1, 1, 0, 0, 0];

你们|昨天|跑步:[0, 0, 0, 0, 0, 1, 1, 1];

你们|又|去|爬山|又|去|跑步=[0, 2, 2, 1, 0, 1, 0, 1];

```shell
你觉得下面哪种情况需要使用稀疏矩阵的存储方式?ABC
A. 有一个1000*1000的矩阵,90%以上的值为0
B. 利用count vector来表示文本的向量
C. 表示一个包含1000个节点的社交网络
D. 表示一个1024*800像素的图片
```

选项C指的是一个社交网络,本质上可以看作是一个图,而且一个人只会跟其中的一小部分人有链接,剩下的没有链接的部分看作是0,所以用稀疏矩阵;

并不是单词出现次数越多越重要，越少越不重要；

所以，如果只记录单词的个数也是不够的，我们还需要考虑单词的权重，也可以认为是质量。这有点类似于，一个人有很多朋友不代表这个人有多厉害，还需要社交的质量，其实是同一个道理。 那如何把这种所谓的“质量”引入到表示中呢?答案是tf-idf

## 1.3 tf-idf

$$
tfidf(w)=tf(d,w)*idf(w);\\
tf(d,w)表示文档d中w的词频，idf(w)=log\frac{N}{N(w)}单词的重要性;\\
N:语料库中的文档总数，N(w)：单词w出现在多少文档中;
$$

举例:文档1:今天 上 NLP 课程；文档2:今天 的 课程 有 意思；文档3:数据 课程 也 有 意思；

则N=3，词库:[今天，上，NLP，课程，的，有，意思，数据，也]；

"文档1:今天 上 NLP 课程"的向量表示如下

==>$tfidf=[1*log\frac{3}{2},1*log\frac{3}{1},1*log\frac{3}{1},1*log\frac{3}{3},0,0,0,0,0]$

"文档2:今天 的 课程 有 意思"的向量表示如下

==>$tfidf=[1*log\frac{3}{2},0,0,1*log\frac{3}{3},1*log\frac{3}{1},1*log\frac{3}{2},1*log\frac{3}{2},0,0]$;

"文档3:数据 课程 也 有 意思"的向量表示如下

==>$tfid=[0,0,0,1*log\frac{3}{3},0,1*log\frac{3}{2},1*log\frac{3}{2},1*log\frac{3}{1},1*log\frac{3}{1}]$;

# 二、文本相似度--欧式距离和余弦相似度

如何计算两个文本之间的相似度?这个问题实际上可以认为是计算两个向量之间的相似度。

**欧式距离没有把方向考虑进去;**

通过余弦相似度事实上我们计算的是两个向量之间的夹角大小。两个向量的方向上越一致就说明它俩的相似度就越高。

余弦相似度公式:$d=\frac{s1*s2}{|s1||s2|}$;

```shell
A. 在分母上除以向量大小是为了消除两个向量大小所带来的影响
B. 内积可用来计算向量之间的相似度
C. 余弦相似度考虑到了方向,欧式距离则没有
D. 在向量之间相似度计算上,余弦相似度的应用相比欧式距离更广泛一些
```

# 三、词向量

## 3.1 计算单词之间的相似度

对于onehot表示的单词，如何计算他们之间的相似度？

我们:[1,0,0,0,0,0,0];爬山:[0,0,1,0,0,0,0];运动:[0,0,0,0,0,0,1];昨天:[0,0,0,0,0,1,0];

欧式距离相似度:
$$
d(我们,爬山)=\sqrt2;d(我们,运动)=\sqrt2;d(爬上,运动)=\sqrt2;...\\
$$
余弦相似度:
$$
d(我们,爬山)=0;d(我们,运动)=0;d(爬上,运动)=0;...\\
$$
通过欧式距离或者余弦相似度是没有办法算出上述单词之间的相似度，因为不管我们怎么计算，俩俩之间的结果都是一样的。

**onehot表示缺点**:

1、每个单词表示成长度为|V|的向量，|V|是词库的大小；

2、无法比较语义相似度；

3、存在稀疏性的问题；

## 3.2 词向量基础

从onehot到**wordvector(分布式表示)**:

我们:[1,0,0,0,0,0,0]==>我们:[0.1,0.2,0.4,0.2]

爬山:[0,0,1,0,0,0,0]==>爬山:[0.2,0.3,0.7,0.1]

运动:[0,0,0,0,0,0,1]==>运动:[0.2,0.3,0.6,0.2]

昨天:[0,0,0,0,0,1,0]==>昨天:[0.5,0.9,0.1,0.3]

计算wordvector分布式表示的单词之间欧式距离：
$$
d(我们,爬上)=\sqrt{0.1^2+0.1^2+0.3^2+0.1^2}=\sqrt{0.12};\\
d(运动,昨天)=\sqrt{0.3^2+0.6^2+0.5^2+0.1^2}=\sqrt{0.71};\\
$$
我们可以看到在分布式表示方法下，两个单词之间的相似度是可以算出来的。当然，效果取决于词向量的质量。

## 3.3 句子向量

最简单的方式:平均；

## 3.4 总结

- 单词的独热编码和分布式表示是两种完全不一样的编码方式；
- 这两种不同的编码方式是目前文本表示的两个方向，有些时候传统的独热编码的方式可能更适合，有些时候分布式表示法更适合，具体还是要通过测试来获得结论；
- 独热编码的最大的问题是不能表示一个单词的含义；
- 词向量的质量取决于词向量训练模型，不同的模型所给出的结果是不一样的；

